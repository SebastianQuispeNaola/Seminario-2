{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pocovidnet.evaluate_covid19 import Evaluator\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import paths\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, balanced_accuracy_score,\n",
    "    accuracy_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, recall_score\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    2: ['covid', 'regular']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_model_val = {\n",
    "    'data': './image_cross_val',\n",
    "    'weights': './trained_models/model_16/val',\n",
    "    'm_id': 'vgg_16',\n",
    "    'classes': 2,\n",
    "    'folds': 5,\n",
    "    'save_path': 'results_vgg',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcc_multiclass(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MCC score for multiclass problem\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    mcc_out = []\n",
    "    for classe in np.unique(y_true):\n",
    "        y_true_binary = (y_true == classe).astype(int)\n",
    "        y_pred_binary = (y_pred == classe).astype(int)\n",
    "        mcc_out.append(matthews_corrcoef(y_true_binary, y_pred_binary))\n",
    "    return mcc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute specificity for multiclass predictions\n",
    "    \"\"\"\n",
    "    # true negatives / negatives\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    spec_out = []\n",
    "    for classe in np.unique(y_true):\n",
    "        negatives = np.sum((y_true != classe).astype(int))\n",
    "        tn = np.sum((y_pred[y_true != classe] != classe).astype(int))\n",
    "        spec_out.append(tn / negatives)\n",
    "    return spec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoding(imagePath, FEAT_ID=0, w=7, h=9, pool=True):\n",
    "    data = np.load(imagePath)\n",
    "\n",
    "    if FEAT_ID > 0 and FEAT_ID < 4:\n",
    "        feats = [FEAT_ID]\n",
    "    elif type(FEAT_ID) != int:\n",
    "        raise TypeError('Give int as feature type')\n",
    "    else:\n",
    "        feats = [1, 2, 3]\n",
    "\n",
    "    sample = []\n",
    "    for feat in feats:\n",
    "        # Use individual features\n",
    "        image = cv2.resize(data['f' + str(feat)][0, :, :, :], (w, h))\n",
    "        sp = np.mean(image, axis=(0, 1)) if pool else image.flatten()\n",
    "        sample.append(sp)\n",
    "    sample = np.concatenate(sample).flatten()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_logits(saved_logits, saved_gt, saved_files, CLASSES, save_path):\n",
    "    all_reports, accs, bal_accs = [], [], []\n",
    "    for s in range(5):\n",
    "        gt_s = saved_gt[s]\n",
    "        pred_idx_s = np.argmax(np.array(saved_logits[s]), axis=1) # p.e [-3.851456  , -0.02610361, -5.3998914 ]: -0.02610361\n",
    "        report = classification_report(\n",
    "            gt_s, pred_idx_s, target_names=CLASSES, output_dict=True\n",
    "        )\n",
    "        mcc_scores = mcc_multiclass(gt_s, pred_idx_s)\n",
    "        spec_scores = specificity(gt_s, pred_idx_s)\n",
    "        for i, cl in enumerate(CLASSES):\n",
    "            report[cl][\"mcc\"] = mcc_scores[i]\n",
    "            report[cl][\"specificity\"] = spec_scores[i]\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df = df.drop(columns=\"support\")\n",
    "        df[\"accuracy\"] = [report[\"accuracy\"] for _ in range(len(df))]\n",
    "        bal = balanced_accuracy_score(gt_s, pred_idx_s)\n",
    "        df[\"balanced\"] = [bal for _ in range(len(df))]\n",
    "        accs.append(report[\"accuracy\"])\n",
    "        bal_accs.append(balanced_accuracy_score(gt_s, pred_idx_s))\n",
    "        all_reports.append(np.array(df)[:len(CLASSES)])\n",
    "        \n",
    "    print(\"Average scores in cross validation:\")\n",
    "    df_arr = np.around(np.mean(all_reports, axis=0), 3)\n",
    "    df_classes = pd.DataFrame(\n",
    "        df_arr,\n",
    "        columns=[\n",
    "            \"Precision\", \"Recall\", \"F1-score\", \"MCC\", \"Specificity\",\n",
    "            \"Accuracy\", \"Balanced\"\n",
    "        ],\n",
    "        index=CLASSES\n",
    "    )\n",
    "    print(df_classes)\n",
    "    df_classes.to_csv(save_path + \"_mean.csv\")\n",
    "\n",
    "    print(\"Standard deviations:\")\n",
    "    df_std = np.around(np.std(all_reports, axis=0), 2)\n",
    "    df_std = pd.DataFrame(\n",
    "        df_std,\n",
    "        columns=[\n",
    "            \"Precision\", \"Recall\", \"F1-score\", \"MCC\", \"Specificity\",\n",
    "            \"Accuracy\", \"Balanced\"\n",
    "        ],\n",
    "        index=CLASSES\n",
    "    )\n",
    "    df_std.to_csv(save_path + \"_std.csv\")\n",
    "    print(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_3(saved_logits, saved_gt, saved_files, CLASSES, save_path):\n",
    "    new_logits, new_gt, new_files = [], [], []\n",
    "    counter = 0\n",
    "    for i in range(5):\n",
    "        gt_inds = np.where(np.array(saved_gt[i]) < 3)[0]\n",
    "        counter += len(gt_inds)\n",
    "        new_logits.append(np.array(saved_logits[i])[gt_inds, :3])\n",
    "        new_gt.append(np.array(saved_gt[i])[gt_inds])\n",
    "        new_files.append(np.array(saved_files[i])[gt_inds])\n",
    "\n",
    "    with open(save_path + \"_3.dat\", \"wb\") as outfile:\n",
    "        pickle.dump((new_logits, new_gt, new_files), outfile)\n",
    "    evaluate_logits(new_logits, new_gt, new_files, CLASSES, save_path + \"_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_logits, saved_gt, saved_files = [], [], []\n",
    "\n",
    "CLASSES = CLASS_MAPPING[args_model_val['classes']]\n",
    "MOD_FILE_MAP = {\n",
    "    'vgg_16': ['gif', 'jpg', 'png', 'peg'],\n",
    "    'vgg_cam': ['jpg', 'png', 'peg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./image_cross_val\\\\split1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(args_model_val['data'], 'split' + str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_models/model_16/val'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_model_val['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(args_model_val['folds']):\n",
    "        print(\"------------- SPLIT \", i, \"-------------------\")\n",
    "        # define data input path\n",
    "        path = os.path.join(args_model_val['data'], 'split'+ str(i))\n",
    "\n",
    "        test_labels, test_files = [], []\n",
    "        test_data = []\n",
    "        \n",
    "        # loop over the image paths (train and test)\n",
    "        for imagePath in paths.list_files(path):\n",
    "            # ejemplo:\n",
    "            # ./image_cross_val/split0/pneumonia/Pneu-grep-bacterial-hepatization-clinical.mp4_frame192.jpg\n",
    "            # select the correct files/ Busca una extensión definida en MOD_FILE_MAP\n",
    "            # Si la extensión de la imagen (.jpg) no está en MOD_...\n",
    "            if imagePath[-3:] not in MOD_FILE_MAP[args_model_val['m_id']]:\n",
    "                continue\n",
    "\n",
    "            # extract the class label from the filename\n",
    "            label_class = imagePath.split(os.path.sep)[-2] # covid, neumonía, regular\n",
    "\n",
    "            # load the image\n",
    "            if args_model_val['m_id'] == 'dense':\n",
    "                image = load_encoding(imagePath)\n",
    "            elif imagePath[-3:] == 'gif':\n",
    "                cap = cv2.VideoCapture(imagePath)\n",
    "                ret, image = cap.read()\n",
    "            else:\n",
    "                image = cv2.imread(imagePath)\n",
    "            \n",
    "            # update the data and labels lists, respectively\n",
    "            test_labels.append(label_class)\n",
    "            test_data.append(image)\n",
    "            test_files.append(imagePath.split(os.path.sep)[-1]) # nombre de la imágen con su extensión (Pneu_liftl_pneu_case3_clip4.mp4_frame30.jpg)\n",
    "            \n",
    "        if args_model_val['m_id'] == 'dense':\n",
    "            test_data = np.expand_dims(np.stack(test_data), 1)\n",
    "            preprocess = False\n",
    "        else:\n",
    "            preprocess = True\n",
    "                \n",
    "        # build ground truth data\n",
    "        gt_class_idx = np.array([CLASSES.index(lab) for lab in test_labels])\n",
    "        model = None\n",
    "        # load model\n",
    "        model = Evaluator(\n",
    "            weights_dir=args_model_val['weights'],\n",
    "            ensemble=False,\n",
    "            split=i,\n",
    "            num_classes=len(CLASSES),\n",
    "            model_id=args_model_val['m_id']\n",
    "        )\n",
    "        print(\"testing on n_files:\", len(test_data))\n",
    "        \n",
    "        # MAIN STEP: feed through model and compute logits\n",
    "        logits = np.array(\n",
    "            [model(img, preprocess=preprocess) for img in test_data]\n",
    "        )\n",
    "\n",
    "        # remember for evaluation:\n",
    "        saved_logits.append(logits)\n",
    "        saved_gt.append(gt_class_idx)\n",
    "        saved_files.append(test_files)\n",
    "\n",
    "        # output the information\n",
    "        predIdxs = np.argmax(logits, axis=1)\n",
    "\n",
    "        print(\n",
    "            classification_report(\n",
    "                gt_class_idx, predIdxs, target_names=CLASSES\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(args_model_val['save_path'] + \".dat\", \"wb\") as outfile:\n",
    "        pickle.dump((saved_logits, saved_gt, saved_files), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE\n",
    "evaluate_logits(\n",
    "    saved_logits, saved_gt, saved_files, CLASSES, args_model_val['save_path']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE ONLY 3 CLASSES\n",
    "if len(CLASSES) == 4:\n",
    "    evaluate_3(\n",
    "        saved_logits, saved_gt, saved_files, CLASS_MAPPING[3],\n",
    "        args.save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_vgg.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum up confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cms = np.zeros((5,3,3))\n",
    "for s in range(5):\n",
    "    # print(saved_files[s])\n",
    "    gt_s = saved_gt[s]\n",
    "    pred_idx_s = np.argmax(np.array(saved_logits[s]), axis=1)\n",
    "    assert len(gt_s)==len(pred_idx_s)\n",
    "    cm = np.array(confusion_matrix(gt_s, pred_idx_s))\n",
    "    all_cms[s] = cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Matrix de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.confusion_matrix_plot(all_cms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "plt.rcParams['legend.title_fontsize'] = 20\n",
    "\n",
    "# plt.rcParams['axes.facecolor'] = 'white'\n",
    "# activate latex text rendering\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"results_vgg.dat\", \"rb\") as outfile:\n",
    "    (saved_logits, saved_gt, saved_files) = pickle.load(outfile)\n",
    "data, max_points, scores, roc_auc_std = Plots.roc_auc(saved_logits, saved_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"red\", \"orange\", \"green\"]\n",
    "classes = [\"COVID-19\", \"Healthy\"]\n",
    "classes_sp =[\"COVID-19\", \"Saludable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC class comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.roc_sensibility_comparison(cols, classes_sp, data, max_points, scores, roc_auc_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
